{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.insert(0, os.pardir)\n",
    "from VocabModel import MyVocabModel\n",
    "from src.Tokenizer import MyTokenizer\n",
    "from src.Preprocessing import preprocessing_dataframe\n",
    "from src.DataLoader import DataLoaderBert\n",
    "from src.Model import MyBert\n",
    "from src.Callback import EarlyStopping\n",
    "from src.Training import model_train, model_eval\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 23\n",
    "NUM_LABELS = 5\n",
    "REM_STOP_WORDS = False\n",
    "\n",
    "COLUMNS_XLSX = ['Vocab_size', 'Time', 'Train_loss', 'Train_acc', 'Val_loss', 'Val_acc']\n",
    "\n",
    "DATA_PATH = os.path.join(os.pardir, os.path.join(\"data\", \"datos.xlsx\"))\n",
    "DATA_SIN = os.path.join(os.pardir, os.path.join(\"data\", \"datos_sinonimos.xlsx\"))\n",
    "NORMAL_PATH = \"vocab_file.txt\"\n",
    "SAVE_PATH_R = \"resultados_vocab.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_XLSX = False\n",
    "if CLEAN_XLSX:\n",
    "    data_frame = pd.DataFrame(columns=COLUMNS_XLSX)\n",
    "    data_frame.to_excel(SAVE_PATH_R, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6052, 2), (2220, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(DATA_PATH)\n",
    "column_name_rev = df.columns.to_list()[len(df.columns.to_list())-1]\n",
    "columns_to_keep = df.columns.to_list()[:2]\n",
    "\n",
    "df_revisado = df[df[column_name_rev] == 'Revisado'][columns_to_keep]\n",
    "df_revisado_eq = preprocessing_dataframe(df_revisado,REM_STOP_WORDS)\n",
    "df_revisado.shape, df_revisado_eq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33015, 2), (35235, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ext = pd.read_excel(DATA_SIN)[columns_to_keep]\n",
    "df_ext_eq = preprocessing_dataframe(df_ext, False)\n",
    "\n",
    "df_eq = pd.concat([df_revisado_eq, df_ext_eq], axis=0)\n",
    "\n",
    "df_ext_eq.shape, df_eq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24664, 5285, 5286)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train , df_test = train_test_split(df_eq, test_size=0.3, random_state = RANDOM_SEED)\n",
    "df_val , df_test = train_test_split(df_test, test_size=0.5, random_state = RANDOM_SEED)\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de la frase: 31, Longitud frase por palabras: 6\n",
      "Longitud máxima de frase en el corpus: 464\n"
     ]
    }
   ],
   "source": [
    "df_rev = pd.concat([df_revisado, df_ext], axis=0)\n",
    "vocab_transform = MyVocabModel(df_rev['Review'])\n",
    "\n",
    "def max_len_corpus():\n",
    "    max_len = 0\n",
    "    for rev in df_revisado['Review']:\n",
    "        n = len(vocab_transform.transform_txt(rev))\n",
    "        if n > max_len:\n",
    "            max_len = n\n",
    "    \n",
    "    return max_len\n",
    "\n",
    "frase = \"no me gustaria puntuar tan bajo\"\n",
    "frase_split = vocab_transform.transform_txt(frase)\n",
    "\n",
    "#Tiene en cuenta los espacios para la longitud\n",
    "print(f\"Longitud de la frase: {len(frase)}, Longitud frase por palabras: {len(frase_split)}\")\n",
    "\n",
    "#Si la longitud maxima es 512, en el mejor de los casos, cuando cada token es una palabra.\n",
    "#Todos los comentarios se podrian representar con toda la información. \n",
    "print(f\"Longitud máxima de frase en el corpus: {max_len_corpus()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE\n",
      "Num epoch: 1, Train_loss: 1.5801771372856352\n",
      "Num epoch: 2, Train_loss: 1.5578734594005446\n",
      "Num epoch: 3, Train_loss: 1.5526873993285863\n",
      "Num epoch: 4, Train_loss: 1.5504395907338162\n",
      "Num epoch: 5, Train_loss: 1.5508423595992227\n",
      "Num epoch: 6, Train_loss: 1.5480998216648207\n",
      "Num epoch: 7, Train_loss: 1.547372654868224\n",
      "Num epoch: 8, Train_loss: 1.545501442725806\n",
      "Num epoch: 9, Train_loss: 1.5452126778451956\n",
      "Num epoch: 10, Train_loss: 1.5428284219805388\n",
      "Num epoch: 11, Train_loss: 1.5426647584719817\n",
      "Num epoch: 12, Train_loss: 1.541228585010452\n",
      "Num epoch: 13, Train_loss: 1.5378589642790281\n",
      "Num epoch: 14, Train_loss: 1.538886215444745\n",
      "Num epoch: 15, Train_loss: 1.5389981310049745\n",
      "Num epoch: 16, Train_loss: 1.5362272997432973\n",
      "Num epoch: 17, Train_loss: 1.5350179427094635\n",
      "Num epoch: 18, Train_loss: 1.5313107954810048\n",
      "Num epoch: 19, Train_loss: 1.5336907877962271\n",
      "Num epoch: 20, Train_loss: 1.5376474498697121\n",
      "Num epoch: 21, Train_loss: 1.5347779531719388\n",
      "Num epoch: 22, Train_loss: 1.5281564048308658\n",
      "Num epoch: 23, Train_loss: 1.5211890730577902\n",
      "Num epoch: 24, Train_loss: 1.5187697983719952\n",
      "Num epoch: 25, Train_loss: 1.5092853647893874\n",
      "Num epoch: 26, Train_loss: 1.5101856572110661\n",
      "Num epoch: 27, Train_loss: 1.5188992538024495\n",
      "Num epoch: 28, Train_loss: 1.4916832992928468\n",
      "Num epoch: 29, Train_loss: 1.4781624352317178\n",
      "Num epoch: 30, Train_loss: 1.4823896760411592\n",
      "Num epoch: 31, Train_loss: 1.4715333887597317\n",
      "Num epoch: 32, Train_loss: 1.461871721795384\n",
      "Num epoch: 33, Train_loss: 1.4513350297182441\n",
      "Num epoch: 34, Train_loss: 1.4315595121167908\n",
      "Num epoch: 35, Train_loss: 1.4201366024945528\n",
      "Num epoch: 36, Train_loss: 1.408370154015321\n",
      "Num epoch: 37, Train_loss: 1.4447315036138852\n",
      "Num epoch: 38, Train_loss: 1.4442091121897465\n",
      "Num epoch: 39, Train_loss: 1.4276008816835286\n",
      "Num epoch: 40, Train_loss: 1.4280181567810242\n",
      "Num epoch: 41, Train_loss: 1.399209854981167\n",
      "Num epoch: 42, Train_loss: 1.3801549193613953\n",
      "Num epoch: 43, Train_loss: 1.3636800277177057\n",
      "Num epoch: 44, Train_loss: 1.3622370220496018\n",
      "Num epoch: 45, Train_loss: 1.3540316585703907\n",
      "Num epoch: 46, Train_loss: 1.3686248320401044\n",
      "Num epoch: 47, Train_loss: 1.377065501560977\n",
      "Num epoch: 48, Train_loss: 1.3761867384824744\n",
      "Num epoch: 49, Train_loss: 1.3693815206446085\n",
      "Num epoch: 50, Train_loss: 1.3487887404595649\n",
      "Num epoch: 51, Train_loss: 1.3467960030763635\n",
      "Num epoch: 52, Train_loss: 1.3191615972701767\n",
      "Num epoch: 53, Train_loss: 1.3044360443167908\n",
      "Num epoch: 54, Train_loss: 1.298062786249673\n",
      "Num epoch: 55, Train_loss: 1.3433418191976645\n",
      "Num epoch: 56, Train_loss: 1.3328162107642039\n",
      "Num epoch: 57, Train_loss: 1.3295717471597297\n",
      "Num epoch: 58, Train_loss: 1.334162930685796\n",
      "Num epoch: 59, Train_loss: 1.326395554080765\n",
      "Num epoch: 60, Train_loss: 1.2874235841329322\n",
      "Num epoch: 61, Train_loss: 1.2888048573883757\n",
      "Num epoch: 62, Train_loss: 1.2454795576393378\n",
      "Num epoch: 63, Train_loss: 1.2377950962235806\n",
      "Num epoch: 64, Train_loss: 1.2254120434196065\n",
      "Num epoch: 65, Train_loss: 1.2274456696800509\n",
      "Num epoch: 66, Train_loss: 1.2086998227091963\n",
      "Num epoch: 67, Train_loss: 1.237517670527381\n",
      "Num epoch: 68, Train_loss: 1.2554303621423155\n",
      "Num epoch: 69, Train_loss: 1.2110686309139014\n",
      "Num epoch: 70, Train_loss: 1.1789124486759484\n",
      "Num epoch: 71, Train_loss: 1.174853884563952\n",
      "Num epoch: 72, Train_loss: 1.169970165259056\n",
      "Num epoch: 73, Train_loss: 1.166568788362983\n",
      "Num epoch: 74, Train_loss: 1.2057550606328995\n",
      "Num epoch: 75, Train_loss: 1.1897004687774382\n",
      "------------------------------\n",
      "Modelo 0 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5746688911503413\n",
      "Num epoch: 2, Train_loss: 1.5503855625259926\n",
      "Num epoch: 3, Train_loss: 1.5493398558637208\n",
      "Num epoch: 4, Train_loss: 1.5449983010885973\n",
      "Num epoch: 5, Train_loss: 1.5427657518338893\n",
      "Num epoch: 6, Train_loss: 1.5411367907158322\n",
      "Num epoch: 7, Train_loss: 1.5402839033587428\n",
      "Num epoch: 8, Train_loss: 1.539580768640653\n",
      "Num epoch: 9, Train_loss: 1.5382540680992498\n",
      "Num epoch: 10, Train_loss: 1.5362341662351087\n",
      "Num epoch: 11, Train_loss: 1.5345066972378976\n",
      "Num epoch: 12, Train_loss: 1.5350480063700884\n",
      "Num epoch: 13, Train_loss: 1.532715646218372\n",
      "Num epoch: 14, Train_loss: 1.533008977736972\n",
      "Num epoch: 15, Train_loss: 1.532909038012344\n",
      "Num epoch: 16, Train_loss: 1.5318253562990196\n",
      "Num epoch: 17, Train_loss: 1.529136792458461\n",
      "Num epoch: 18, Train_loss: 1.5284408897770483\n",
      "Num epoch: 19, Train_loss: 1.5296269006111234\n",
      "Num epoch: 20, Train_loss: 1.5305953832694241\n",
      "Num epoch: 21, Train_loss: 1.5198710031162173\n",
      "Num epoch: 22, Train_loss: 1.5155840507583729\n",
      "Num epoch: 23, Train_loss: 1.5170213315554961\n",
      "Num epoch: 24, Train_loss: 1.5010107891477626\n",
      "Num epoch: 25, Train_loss: 1.4880553016483185\n",
      "Num epoch: 26, Train_loss: 1.4872431540320996\n",
      "Num epoch: 27, Train_loss: 1.47885813872187\n",
      "Num epoch: 28, Train_loss: 1.4646014693086502\n",
      "Num epoch: 29, Train_loss: 1.4581656318533702\n",
      "Num epoch: 30, Train_loss: 1.4628777939740458\n",
      "Num epoch: 31, Train_loss: 1.4484894756291773\n",
      "Num epoch: 32, Train_loss: 1.4269923542058587\n",
      "Num epoch: 33, Train_loss: 1.4320244008347156\n",
      "Num epoch: 34, Train_loss: 1.4283934569797692\n",
      "Num epoch: 35, Train_loss: 1.4140439139793262\n",
      "Num epoch: 36, Train_loss: 1.416144842533875\n",
      "Num epoch: 37, Train_loss: 1.4256962379691394\n",
      "Num epoch: 38, Train_loss: 1.424750485439657\n",
      "Num epoch: 39, Train_loss: 1.4113611064916534\n",
      "Num epoch: 40, Train_loss: 1.4509515104609343\n",
      "Num epoch: 41, Train_loss: 1.4328830497501888\n",
      "Num epoch: 42, Train_loss: 1.4146341747600384\n",
      "Num epoch: 43, Train_loss: 1.376672379950444\n",
      "Num epoch: 44, Train_loss: 1.358423954963701\n",
      "Num epoch: 45, Train_loss: 1.3616296327345139\n",
      "Num epoch: 46, Train_loss: 1.340497680726161\n",
      "Num epoch: 47, Train_loss: 1.329896018638883\n",
      "Num epoch: 48, Train_loss: 1.3192866441005\n",
      "Num epoch: 49, Train_loss: 1.3266136622877427\n",
      "Num epoch: 50, Train_loss: 1.3222439458936288\n",
      "Num epoch: 51, Train_loss: 1.348294155045672\n",
      "Num epoch: 52, Train_loss: 1.325170695742437\n",
      "Num epoch: 53, Train_loss: 1.33483253205041\n",
      "Num epoch: 54, Train_loss: 1.3094722001420342\n",
      "Num epoch: 55, Train_loss: 1.2939949473193955\n",
      "Num epoch: 56, Train_loss: 1.2790625435670224\n",
      "Num epoch: 57, Train_loss: 1.2645833955813675\n",
      "Num epoch: 58, Train_loss: 1.2734097306276158\n",
      "Num epoch: 59, Train_loss: 1.2706161771142421\n",
      "Num epoch: 60, Train_loss: 1.2439967217597478\n",
      "Num epoch: 61, Train_loss: 1.2070828800609543\n",
      "Num epoch: 62, Train_loss: 1.2289695390272055\n",
      "Num epoch: 63, Train_loss: 1.2467804279237589\n",
      "Num epoch: 64, Train_loss: 1.2240162169620603\n",
      "Num epoch: 65, Train_loss: 1.2335238778323874\n",
      "Num epoch: 66, Train_loss: 1.258482413406312\n",
      "Num epoch: 67, Train_loss: 1.262347907598239\n",
      "Num epoch: 68, Train_loss: 1.3499062710039575\n",
      "Num epoch: 69, Train_loss: 1.2998926214942665\n",
      "Num epoch: 70, Train_loss: 1.2646895978410078\n",
      "Num epoch: 71, Train_loss: 1.3336852599579407\n",
      "Num epoch: 72, Train_loss: 1.3174594155151156\n",
      "Num epoch: 73, Train_loss: 1.3182350494864117\n",
      "------------------------------\n",
      "Modelo 1 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.571794014200544\n",
      "Num epoch: 2, Train_loss: 1.5513302971316822\n",
      "Num epoch: 3, Train_loss: 1.548586823425624\n",
      "Num epoch: 4, Train_loss: 1.547369115844857\n",
      "Num epoch: 5, Train_loss: 1.5450437566675692\n",
      "Num epoch: 6, Train_loss: 1.543821799689363\n",
      "Num epoch: 7, Train_loss: 1.5425649697599648\n",
      "Num epoch: 8, Train_loss: 1.5405765470362622\n",
      "Num epoch: 9, Train_loss: 1.540493242101858\n",
      "Num epoch: 10, Train_loss: 1.537086931531243\n",
      "Num epoch: 11, Train_loss: 1.5336906171602898\n",
      "Num epoch: 12, Train_loss: 1.531860532077767\n",
      "Num epoch: 13, Train_loss: 1.5332397345285542\n",
      "Num epoch: 14, Train_loss: 1.5324007628260206\n",
      "Num epoch: 15, Train_loss: 1.5324576399831311\n",
      "Num epoch: 16, Train_loss: 1.530066685836268\n",
      "Num epoch: 17, Train_loss: 1.5282155473511443\n",
      "Num epoch: 18, Train_loss: 1.5286926343152556\n",
      "Num epoch: 19, Train_loss: 1.528962643791088\n",
      "Num epoch: 20, Train_loss: 1.526581215338393\n",
      "Num epoch: 21, Train_loss: 1.5247072324789928\n",
      "Num epoch: 22, Train_loss: 1.5258741573954109\n",
      "Num epoch: 23, Train_loss: 1.5224441942981493\n",
      "Num epoch: 24, Train_loss: 1.5233278817609266\n",
      "Num epoch: 25, Train_loss: 1.5187309862555833\n",
      "Num epoch: 26, Train_loss: 1.516772404714254\n",
      "Num epoch: 27, Train_loss: 1.5126381413119976\n",
      "Num epoch: 28, Train_loss: 1.507829694238685\n",
      "Num epoch: 29, Train_loss: 1.5076362744307232\n",
      "Num epoch: 30, Train_loss: 1.516380904191042\n",
      "Num epoch: 31, Train_loss: 1.5178441287169702\n",
      "Num epoch: 32, Train_loss: 1.5036414008209391\n",
      "Num epoch: 33, Train_loss: 1.4962126902856496\n",
      "Num epoch: 34, Train_loss: 1.4902114472700874\n",
      "Num epoch: 35, Train_loss: 1.4782860487931508\n",
      "Num epoch: 36, Train_loss: 1.467125349474193\n",
      "Num epoch: 37, Train_loss: 1.457846422184608\n",
      "Num epoch: 38, Train_loss: 1.4512974405120496\n",
      "Num epoch: 39, Train_loss: 1.4565763064755892\n",
      "Num epoch: 40, Train_loss: 1.4529829590671681\n",
      "Num epoch: 41, Train_loss: 1.4374522550797022\n",
      "Num epoch: 42, Train_loss: 1.435991066638193\n",
      "Num epoch: 43, Train_loss: 1.4357452380591462\n",
      "Num epoch: 44, Train_loss: 1.4277414244720776\n",
      "Num epoch: 45, Train_loss: 1.4157485545909216\n",
      "Num epoch: 46, Train_loss: 1.4000017297584981\n",
      "Num epoch: 47, Train_loss: 1.4225804911605577\n",
      "Num epoch: 48, Train_loss: 1.4187282625388835\n",
      "Num epoch: 49, Train_loss: 1.3930173826041718\n",
      "Num epoch: 50, Train_loss: 1.3640248441933966\n",
      "Num epoch: 51, Train_loss: 1.3541117410090617\n",
      "Num epoch: 52, Train_loss: 1.3383215547896055\n",
      "Num epoch: 53, Train_loss: 1.3521796791266592\n",
      "Num epoch: 54, Train_loss: 1.3835302033460724\n",
      "Num epoch: 55, Train_loss: 1.3608174380766873\n",
      "Num epoch: 56, Train_loss: 1.3469131535189205\n",
      "Num epoch: 57, Train_loss: 1.3569395660440566\n",
      "Num epoch: 58, Train_loss: 1.36330085360855\n",
      "Num epoch: 59, Train_loss: 1.3686732696589812\n",
      "Num epoch: 60, Train_loss: 1.3791013138342747\n",
      "Num epoch: 61, Train_loss: 1.3485806077003402\n",
      "Num epoch: 62, Train_loss: 1.3333687713219289\n",
      "------------------------------\n",
      "Modelo 2 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5667148918503104\n",
      "Num epoch: 2, Train_loss: 1.5506708221198906\n",
      "Num epoch: 3, Train_loss: 1.547873289330687\n",
      "Num epoch: 4, Train_loss: 1.5448037456797192\n",
      "Num epoch: 5, Train_loss: 1.5431629010426933\n",
      "Num epoch: 6, Train_loss: 1.5439298205606897\n",
      "Num epoch: 7, Train_loss: 1.5414136450688174\n",
      "Num epoch: 8, Train_loss: 1.5432660354544812\n",
      "Num epoch: 9, Train_loss: 1.539440392239543\n",
      "Num epoch: 10, Train_loss: 1.536630446478565\n",
      "Num epoch: 11, Train_loss: 1.5343618904310814\n",
      "Num epoch: 12, Train_loss: 1.5347260384156725\n",
      "Num epoch: 13, Train_loss: 1.5318025621140579\n",
      "Num epoch: 14, Train_loss: 1.5317667611342067\n",
      "Num epoch: 15, Train_loss: 1.5293182333181556\n",
      "Num epoch: 16, Train_loss: 1.529466905243552\n",
      "Num epoch: 17, Train_loss: 1.5271696619716049\n",
      "Num epoch: 18, Train_loss: 1.5256967008055862\n",
      "Num epoch: 19, Train_loss: 1.522105987838364\n",
      "Num epoch: 20, Train_loss: 1.5236927812899084\n",
      "Num epoch: 21, Train_loss: 1.5219156677909305\n",
      "Num epoch: 22, Train_loss: 1.524138068944032\n",
      "Num epoch: 23, Train_loss: 1.5294541395062407\n",
      "Num epoch: 24, Train_loss: 1.5256983927229648\n",
      "Num epoch: 25, Train_loss: 1.5208528768482665\n",
      "Num epoch: 26, Train_loss: 1.5175813494604824\n",
      "Num epoch: 27, Train_loss: 1.518434807199029\n",
      "Num epoch: 28, Train_loss: 1.5199259333493762\n",
      "Num epoch: 29, Train_loss: 1.5170528286565947\n",
      "Num epoch: 30, Train_loss: 1.520606242987849\n",
      "Num epoch: 31, Train_loss: 1.5167043048705986\n",
      "Num epoch: 32, Train_loss: 1.5165421522131532\n",
      "Num epoch: 33, Train_loss: 1.51959092349075\n",
      "Num epoch: 34, Train_loss: 1.5179830741136058\n",
      "Num epoch: 35, Train_loss: 1.5154919806835212\n",
      "Num epoch: 36, Train_loss: 1.5143705930777118\n",
      "Num epoch: 37, Train_loss: 1.5125491622686773\n",
      "Num epoch: 38, Train_loss: 1.5057733467442627\n",
      "Num epoch: 39, Train_loss: 1.4980431381967059\n",
      "Num epoch: 40, Train_loss: 1.4988295338948825\n",
      "Num epoch: 41, Train_loss: 1.4985260491900114\n",
      "Num epoch: 42, Train_loss: 1.4909474903447575\n",
      "Num epoch: 43, Train_loss: 1.493032726284903\n",
      "Num epoch: 44, Train_loss: 1.484047056675112\n",
      "Num epoch: 45, Train_loss: 1.4718909789767314\n",
      "Num epoch: 46, Train_loss: 1.4525583141081293\n",
      "Num epoch: 47, Train_loss: 1.448602590580633\n",
      "Num epoch: 48, Train_loss: 1.444790848766756\n",
      "Num epoch: 49, Train_loss: 1.4349656668704631\n",
      "Num epoch: 50, Train_loss: 1.4183244962236767\n",
      "Num epoch: 51, Train_loss: 1.4056766566095744\n",
      "Num epoch: 52, Train_loss: 1.4092661889582303\n",
      "Num epoch: 53, Train_loss: 1.3814701410810977\n",
      "Num epoch: 54, Train_loss: 1.3741134749897648\n",
      "Num epoch: 55, Train_loss: 1.3697758282894559\n",
      "Num epoch: 56, Train_loss: 1.3511804766298074\n",
      "Num epoch: 57, Train_loss: 1.333055642885683\n",
      "Num epoch: 58, Train_loss: 1.32780475793645\n",
      "Num epoch: 59, Train_loss: 1.334134708253828\n",
      "Num epoch: 60, Train_loss: 1.3056541693783834\n",
      "Num epoch: 61, Train_loss: 1.3123987694785961\n",
      "Num epoch: 62, Train_loss: 1.3203173822376235\n",
      "Num epoch: 63, Train_loss: 1.3137808339818322\n",
      "Num epoch: 64, Train_loss: 1.3102224057119194\n",
      "Num epoch: 65, Train_loss: 1.2856730899114694\n",
      "Num epoch: 66, Train_loss: 1.2490772078906682\n",
      "Num epoch: 67, Train_loss: 1.2329438026671842\n",
      "Num epoch: 68, Train_loss: 1.2354334094001271\n",
      "Num epoch: 69, Train_loss: 1.2353461587634222\n",
      "Num epoch: 70, Train_loss: 1.2408590264802064\n",
      "Num epoch: 71, Train_loss: 1.2524257704332844\n",
      "Num epoch: 72, Train_loss: 1.2120394381902027\n",
      "Num epoch: 73, Train_loss: 1.2279649081824517\n",
      "Num epoch: 74, Train_loss: 1.2122076165367266\n",
      "Num epoch: 75, Train_loss: 1.200767724444601\n",
      "Num epoch: 76, Train_loss: 1.2096999029529547\n",
      "Num epoch: 77, Train_loss: 1.208332216594292\n",
      "Num epoch: 78, Train_loss: 1.1828680366574944\n",
      "Num epoch: 79, Train_loss: 1.1800476029845142\n",
      "Num epoch: 80, Train_loss: 1.1909895354393716\n",
      "Num epoch: 81, Train_loss: 1.1462550960460625\n",
      "Num epoch: 82, Train_loss: 1.1162147196619585\n",
      "Num epoch: 83, Train_loss: 1.129529778627051\n",
      "Num epoch: 84, Train_loss: 1.1385586663458027\n",
      "Num epoch: 85, Train_loss: 1.105218398740453\n",
      "Num epoch: 86, Train_loss: 1.1144147599557008\n",
      "Num epoch: 87, Train_loss: 1.0815059434881713\n",
      "Num epoch: 88, Train_loss: 1.0541009578937373\n",
      "Num epoch: 89, Train_loss: 1.0460626514353786\n",
      "Num epoch: 90, Train_loss: 1.0652258721040697\n",
      "Num epoch: 91, Train_loss: 1.0653041723669736\n",
      "Num epoch: 92, Train_loss: 1.051336124852711\n",
      "Num epoch: 93, Train_loss: 1.044469490700407\n",
      "Num epoch: 94, Train_loss: 1.0781991436121128\n",
      "Num epoch: 95, Train_loss: 1.0864463938944504\n",
      "Num epoch: 96, Train_loss: 1.065515561635294\n",
      "Num epoch: 97, Train_loss: 1.054384366383349\n",
      "Num epoch: 98, Train_loss: 1.0589115010654362\n",
      "Num epoch: 99, Train_loss: 1.0618690823020847\n",
      "Num epoch: 100, Train_loss: 1.0465337617251511\n",
      "Num epoch: 101, Train_loss: 1.025280293310061\n",
      "Num epoch: 102, Train_loss: 1.0575931626056363\n",
      "Num epoch: 103, Train_loss: 1.0724230761982034\n",
      "Num epoch: 104, Train_loss: 1.0283462668947592\n",
      "Num epoch: 105, Train_loss: 1.0584263146087343\n",
      "Num epoch: 106, Train_loss: 1.0534787043783194\n",
      "Num epoch: 107, Train_loss: 1.0396988654546646\n",
      "Num epoch: 108, Train_loss: 0.9993891595751639\n",
      "Num epoch: 109, Train_loss: 0.9802017026259378\n",
      "Num epoch: 110, Train_loss: 0.9662043695627164\n",
      "Num epoch: 111, Train_loss: 1.0033308087968658\n",
      "Num epoch: 112, Train_loss: 0.9395734663573394\n",
      "Num epoch: 113, Train_loss: 0.9228117345780783\n",
      "Num epoch: 114, Train_loss: 0.9694507311891548\n",
      "Num epoch: 115, Train_loss: 0.9477258885193657\n",
      "Num epoch: 116, Train_loss: 0.9227407745387853\n",
      "Num epoch: 117, Train_loss: 0.9463346841379708\n",
      "Num epoch: 118, Train_loss: 1.0482387854929884\n",
      "Num epoch: 119, Train_loss: 1.053703446707897\n",
      "Num epoch: 120, Train_loss: 1.0368684536621562\n",
      "Num epoch: 121, Train_loss: 1.0184560420403654\n",
      "Num epoch: 122, Train_loss: 1.0043034883193895\n",
      "Num epoch: 123, Train_loss: 1.0145324908325009\n",
      "Num epoch: 124, Train_loss: 1.013693529208974\n",
      "------------------------------\n",
      "Modelo 3 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5680635569256465\n",
      "Num epoch: 2, Train_loss: 1.5501640951076654\n",
      "Num epoch: 3, Train_loss: 1.5453211843116772\n",
      "Num epoch: 4, Train_loss: 1.543827204446378\n",
      "Num epoch: 5, Train_loss: 1.5438783194312715\n",
      "Num epoch: 6, Train_loss: 1.5423607254546454\n",
      "Num epoch: 7, Train_loss: 1.5379724915233326\n",
      "Num epoch: 8, Train_loss: 1.5403632503842672\n",
      "Num epoch: 9, Train_loss: 1.5392704408759619\n",
      "Num epoch: 10, Train_loss: 1.538133652903142\n",
      "Num epoch: 11, Train_loss: 1.5393672872683928\n",
      "Num epoch: 12, Train_loss: 1.5379805230123826\n",
      "Num epoch: 13, Train_loss: 1.5351706437290469\n",
      "Num epoch: 14, Train_loss: 1.5337854011218115\n",
      "Num epoch: 15, Train_loss: 1.5324305652547416\n",
      "Num epoch: 16, Train_loss: 1.5308465714331692\n",
      "Num epoch: 17, Train_loss: 1.5307710053295893\n",
      "Num epoch: 18, Train_loss: 1.5304616589342044\n",
      "Num epoch: 19, Train_loss: 1.5294567218972923\n",
      "Num epoch: 20, Train_loss: 1.5281941642379977\n",
      "Num epoch: 21, Train_loss: 1.5298548683190631\n",
      "Num epoch: 22, Train_loss: 1.530041217291622\n",
      "Num epoch: 23, Train_loss: 1.5296340798291532\n",
      "Num epoch: 24, Train_loss: 1.5270336875465447\n",
      "Num epoch: 25, Train_loss: 1.5282389506325489\n",
      "Num epoch: 26, Train_loss: 1.5250082600340997\n",
      "Num epoch: 27, Train_loss: 1.5220384011978536\n",
      "Num epoch: 28, Train_loss: 1.5201980821157561\n",
      "Num epoch: 29, Train_loss: 1.5168899798640716\n",
      "Num epoch: 30, Train_loss: 1.5205916746691996\n",
      "Num epoch: 31, Train_loss: 1.5231530171998167\n",
      "Num epoch: 32, Train_loss: 1.5210767531893759\n",
      "Num epoch: 33, Train_loss: 1.5191039212176136\n",
      "Num epoch: 34, Train_loss: 1.5235687665764557\n",
      "Num epoch: 35, Train_loss: 1.5218276704552496\n",
      "Num epoch: 36, Train_loss: 1.5183912032887104\n",
      "Num epoch: 37, Train_loss: 1.5163761756171121\n",
      "Num epoch: 38, Train_loss: 1.523336447594114\n",
      "Num epoch: 39, Train_loss: 1.5262178717071733\n",
      "Num epoch: 40, Train_loss: 1.52009026848802\n",
      "Num epoch: 41, Train_loss: 1.5011406888870538\n",
      "Num epoch: 42, Train_loss: 1.49698243127061\n",
      "Num epoch: 43, Train_loss: 1.4924911445431457\n",
      "Num epoch: 44, Train_loss: 1.484315645673466\n",
      "Num epoch: 45, Train_loss: 1.472179988646252\n",
      "Num epoch: 46, Train_loss: 1.4675709873031881\n",
      "Num epoch: 47, Train_loss: 1.449481245390092\n",
      "Num epoch: 48, Train_loss: 1.4400310510895757\n",
      "Num epoch: 49, Train_loss: 1.4491887164872805\n",
      "Num epoch: 50, Train_loss: 1.4452152668540446\n",
      "Num epoch: 51, Train_loss: 1.4370535338443091\n",
      "Num epoch: 52, Train_loss: 1.4291639833617744\n",
      "Num epoch: 53, Train_loss: 1.4149340229870784\n",
      "Num epoch: 54, Train_loss: 1.4015368220628641\n",
      "Num epoch: 55, Train_loss: 1.3998777669018598\n",
      "Num epoch: 56, Train_loss: 1.4054895095331421\n",
      "Num epoch: 57, Train_loss: 1.3877275125076738\n",
      "Num epoch: 58, Train_loss: 1.391391802773706\n",
      "Num epoch: 59, Train_loss: 1.3911440047908827\n",
      "Num epoch: 60, Train_loss: 1.3748504012832772\n",
      "Num epoch: 61, Train_loss: 1.370560943804853\n",
      "Num epoch: 62, Train_loss: 1.3920698050763598\n",
      "Num epoch: 63, Train_loss: 1.3716884741086561\n",
      "Num epoch: 64, Train_loss: 1.3543779927701909\n",
      "Num epoch: 65, Train_loss: 1.3485433140330372\n",
      "Num epoch: 66, Train_loss: 1.3543367112974358\n",
      "Num epoch: 67, Train_loss: 1.3923748118988384\n",
      "Num epoch: 68, Train_loss: 1.368645649252292\n",
      "------------------------------\n",
      "Modelo 4 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5687982241151552\n",
      "Num epoch: 2, Train_loss: 1.5508748661996172\n",
      "Num epoch: 3, Train_loss: 1.5471120880789235\n",
      "Num epoch: 4, Train_loss: 1.5439066234982095\n",
      "Num epoch: 5, Train_loss: 1.5424973980157823\n",
      "Num epoch: 6, Train_loss: 1.5420179036272217\n",
      "Num epoch: 7, Train_loss: 1.5410308264174168\n",
      "Num epoch: 8, Train_loss: 1.5393332310439007\n",
      "Num epoch: 9, Train_loss: 1.5351437862685775\n",
      "Num epoch: 10, Train_loss: 1.5332398814618335\n",
      "Num epoch: 11, Train_loss: 1.5325734594735936\n",
      "Num epoch: 12, Train_loss: 1.5355557824085952\n",
      "Num epoch: 13, Train_loss: 1.5335912005761363\n",
      "Num epoch: 14, Train_loss: 1.5328041485695338\n",
      "Num epoch: 15, Train_loss: 1.5327006079742285\n",
      "Num epoch: 16, Train_loss: 1.5305350874099026\n",
      "Num epoch: 17, Train_loss: 1.5296245233659964\n",
      "Num epoch: 18, Train_loss: 1.530530730190503\n",
      "Num epoch: 19, Train_loss: 1.5265111263661348\n",
      "Num epoch: 20, Train_loss: 1.5255253909665039\n",
      "Num epoch: 21, Train_loss: 1.5261148818313321\n",
      "Num epoch: 22, Train_loss: 1.5223992535946549\n",
      "Num epoch: 23, Train_loss: 1.5242350937733973\n",
      "Num epoch: 24, Train_loss: 1.5204077992120657\n",
      "Num epoch: 25, Train_loss: 1.5160198062609977\n",
      "Num epoch: 26, Train_loss: 1.515043520082563\n",
      "Num epoch: 27, Train_loss: 1.5142117000190694\n",
      "Num epoch: 28, Train_loss: 1.5102465134663818\n",
      "Num epoch: 29, Train_loss: 1.5110254923302826\n",
      "Num epoch: 30, Train_loss: 1.4942415399362698\n",
      "Num epoch: 31, Train_loss: 1.4769295609658544\n",
      "Num epoch: 32, Train_loss: 1.4677830167861485\n",
      "Num epoch: 33, Train_loss: 1.4741090149547695\n",
      "Num epoch: 34, Train_loss: 1.4561680755346889\n",
      "Num epoch: 35, Train_loss: 1.4477499611881155\n",
      "Num epoch: 36, Train_loss: 1.4462195862685168\n",
      "Num epoch: 37, Train_loss: 1.433291956096008\n",
      "Num epoch: 38, Train_loss: 1.416453840934221\n",
      "Num epoch: 39, Train_loss: 1.4103036552566117\n",
      "Num epoch: 40, Train_loss: 1.4096926871422548\n",
      "Num epoch: 41, Train_loss: 1.4230617069619027\n",
      "Num epoch: 42, Train_loss: 1.4060291088887944\n",
      "Num epoch: 43, Train_loss: 1.3878763661506286\n",
      "Num epoch: 44, Train_loss: 1.3897972596417163\n",
      "Num epoch: 45, Train_loss: 1.3776049434418536\n",
      "Num epoch: 46, Train_loss: 1.3832769630198312\n",
      "Num epoch: 47, Train_loss: 1.3969597152609197\n",
      "Num epoch: 48, Train_loss: 1.393742928944197\n",
      "Num epoch: 49, Train_loss: 1.3891851274206621\n",
      "Num epoch: 50, Train_loss: 1.3866107066150148\n",
      "Num epoch: 51, Train_loss: 1.402055537738353\n",
      "Num epoch: 52, Train_loss: 1.3758872344466413\n",
      "Num epoch: 53, Train_loss: 1.3679060776570846\n",
      "Num epoch: 54, Train_loss: 1.3464749887769307\n",
      "Num epoch: 55, Train_loss: 1.3401848469829738\n",
      "Num epoch: 56, Train_loss: 1.3559648280010532\n",
      "Num epoch: 57, Train_loss: 1.3403756369630004\n",
      "Num epoch: 58, Train_loss: 1.3283788635444707\n",
      "Num epoch: 59, Train_loss: 1.3348983225596636\n",
      "Num epoch: 60, Train_loss: 1.3335074006065972\n",
      "Num epoch: 61, Train_loss: 1.330855119708796\n",
      "Num epoch: 62, Train_loss: 1.3353374793139556\n",
      "Num epoch: 63, Train_loss: 1.341175429068658\n",
      "Num epoch: 64, Train_loss: 1.3420111567151747\n",
      "Num epoch: 65, Train_loss: 1.3188471073237016\n",
      "Num epoch: 66, Train_loss: 1.3005778422023941\n",
      "Num epoch: 67, Train_loss: 1.327261240929341\n",
      "Num epoch: 68, Train_loss: 1.2904603265363637\n",
      "Num epoch: 69, Train_loss: 1.2764825939620412\n",
      "Num epoch: 70, Train_loss: 1.267335652362554\n",
      "Num epoch: 71, Train_loss: 1.2398697536787913\n",
      "Num epoch: 72, Train_loss: 1.2213446921232436\n",
      "Num epoch: 73, Train_loss: 1.225042518248834\n",
      "Num epoch: 74, Train_loss: 1.2382436520642046\n",
      "Num epoch: 75, Train_loss: 1.2474493573386822\n",
      "Num epoch: 76, Train_loss: 1.2271033601465016\n",
      "Num epoch: 77, Train_loss: 1.240776138612774\n",
      "Num epoch: 78, Train_loss: 1.2614308186805832\n",
      "Num epoch: 79, Train_loss: 1.260219423219373\n",
      "Num epoch: 80, Train_loss: 1.2289983570537046\n",
      "Num epoch: 81, Train_loss: 1.2030658703968002\n",
      "Num epoch: 82, Train_loss: 1.2050644008921862\n",
      "Num epoch: 83, Train_loss: 1.2015437644211853\n",
      "Num epoch: 84, Train_loss: 1.1896264910307646\n",
      "Num epoch: 85, Train_loss: 1.188988606078158\n",
      "Num epoch: 86, Train_loss: 1.1752756108918647\n",
      "Num epoch: 87, Train_loss: 1.1632576519279023\n",
      "Num epoch: 88, Train_loss: 1.1653411617624008\n",
      "Num epoch: 89, Train_loss: 1.172217448301519\n",
      "Num epoch: 90, Train_loss: 1.1864816224413368\n",
      "Num epoch: 91, Train_loss: 1.1690202244097367\n",
      "Num epoch: 92, Train_loss: 1.1519123855174709\n",
      "Num epoch: 93, Train_loss: 1.1345055730543074\n",
      "Num epoch: 94, Train_loss: 1.124262069634419\n",
      "Num epoch: 95, Train_loss: 1.1294292013387144\n",
      "Num epoch: 96, Train_loss: 1.111450693528088\n",
      "Num epoch: 97, Train_loss: 1.0877951392467036\n",
      "Num epoch: 98, Train_loss: 1.1010827151596294\n",
      "Num epoch: 99, Train_loss: 1.0646944388193194\n",
      "Num epoch: 100, Train_loss: 1.0569284050010974\n",
      "Num epoch: 101, Train_loss: 1.0546117383470754\n",
      "Num epoch: 102, Train_loss: 1.0699639825938505\n",
      "Num epoch: 103, Train_loss: 1.0547750751477942\n",
      "Num epoch: 104, Train_loss: 1.0700666532525922\n",
      "Num epoch: 105, Train_loss: 1.0504412175741613\n",
      "Num epoch: 106, Train_loss: 1.036483330072722\n",
      "Num epoch: 107, Train_loss: 1.0540704222273976\n",
      "Num epoch: 108, Train_loss: 1.0888029593905477\n",
      "Num epoch: 109, Train_loss: 1.0760952411405258\n",
      "Num epoch: 110, Train_loss: 1.04424969921042\n",
      "Num epoch: 111, Train_loss: 1.048943748379998\n",
      "Num epoch: 112, Train_loss: 1.048375314479483\n",
      "Num epoch: 113, Train_loss: 1.0582116983303491\n",
      "Num epoch: 114, Train_loss: 1.0509381133286955\n",
      "Num epoch: 115, Train_loss: 1.0768992173731051\n",
      "Num epoch: 116, Train_loss: 1.0754056037263175\n",
      "Num epoch: 117, Train_loss: 1.099278736711237\n",
      "Num epoch: 118, Train_loss: 1.1024824055105067\n",
      "Num epoch: 119, Train_loss: 1.0973407889473452\n",
      "Num epoch: 120, Train_loss: 1.0867698463843003\n",
      "------------------------------\n",
      "Modelo 5 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5661722124782893\n",
      "Num epoch: 2, Train_loss: 1.5502886386359236\n",
      "Num epoch: 3, Train_loss: 1.5437908579960606\n",
      "Num epoch: 4, Train_loss: 1.5412176464271359\n",
      "Num epoch: 5, Train_loss: 1.539003694652931\n",
      "Num epoch: 6, Train_loss: 1.5307286778828018\n",
      "Num epoch: 7, Train_loss: 1.522358195825401\n",
      "Num epoch: 8, Train_loss: 1.5274392967385748\n",
      "Num epoch: 9, Train_loss: 1.5230801667905727\n",
      "Num epoch: 10, Train_loss: 1.5221002340278105\n",
      "Num epoch: 11, Train_loss: 1.5268998541520318\n",
      "Num epoch: 12, Train_loss: 1.5288624314278934\n",
      "Num epoch: 13, Train_loss: 1.5236653251439922\n",
      "Num epoch: 14, Train_loss: 1.5206111613280426\n",
      "Num epoch: 15, Train_loss: 1.5200015277279455\n",
      "Num epoch: 16, Train_loss: 1.5166216111689699\n",
      "------------------------------\n",
      "Modelo 6 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5670359504810603\n",
      "Num epoch: 2, Train_loss: 1.5494661590919996\n",
      "Num epoch: 3, Train_loss: 1.5465617799890379\n",
      "Num epoch: 4, Train_loss: 1.5417536980062578\n",
      "Num epoch: 5, Train_loss: 1.538770036315748\n",
      "Num epoch: 6, Train_loss: 1.53766351945981\n",
      "Num epoch: 7, Train_loss: 1.5356085814132034\n",
      "Num epoch: 8, Train_loss: 1.535910563580293\n",
      "Num epoch: 9, Train_loss: 1.533519905561988\n",
      "Num epoch: 10, Train_loss: 1.5321825710628236\n",
      "Num epoch: 11, Train_loss: 1.532397628874505\n",
      "Num epoch: 12, Train_loss: 1.5274707954443394\n",
      "Num epoch: 13, Train_loss: 1.5284145073364046\n",
      "Num epoch: 14, Train_loss: 1.5271403467717048\n",
      "Num epoch: 15, Train_loss: 1.5237491635332006\n",
      "Num epoch: 16, Train_loss: 1.5219797054997444\n",
      "Num epoch: 17, Train_loss: 1.519030091836396\n",
      "Num epoch: 18, Train_loss: 1.5208364616072956\n",
      "Num epoch: 19, Train_loss: 1.5193793012111403\n",
      "Num epoch: 20, Train_loss: 1.5198231806837812\n",
      "Num epoch: 21, Train_loss: 1.5120938067945844\n",
      "Num epoch: 22, Train_loss: 1.5120428314736398\n",
      "Num epoch: 23, Train_loss: 1.5173601393300606\n",
      "Num epoch: 24, Train_loss: 1.5149095166127788\n",
      "Num epoch: 25, Train_loss: 1.5067323795530343\n",
      "Num epoch: 26, Train_loss: 1.4965781999744072\n",
      "Num epoch: 27, Train_loss: 1.4926921298586622\n",
      "Num epoch: 28, Train_loss: 1.486414363428327\n",
      "Num epoch: 29, Train_loss: 1.4900701651509767\n",
      "Num epoch: 30, Train_loss: 1.479733829331251\n",
      "Num epoch: 31, Train_loss: 1.4693500986549324\n",
      "Num epoch: 32, Train_loss: 1.4635088549122732\n",
      "Num epoch: 33, Train_loss: 1.4559539517884976\n",
      "Num epoch: 34, Train_loss: 1.4410054489868707\n",
      "Num epoch: 35, Train_loss: 1.4247105928972252\n",
      "Num epoch: 36, Train_loss: 1.4187849191897417\n",
      "Num epoch: 37, Train_loss: 1.4149727190431163\n",
      "Num epoch: 38, Train_loss: 1.398795180547133\n",
      "Num epoch: 39, Train_loss: 1.4097376981223901\n",
      "Num epoch: 40, Train_loss: 1.3918282533287423\n",
      "Num epoch: 41, Train_loss: 1.3924367046621067\n",
      "Num epoch: 42, Train_loss: 1.3697776733485283\n",
      "Num epoch: 43, Train_loss: 1.3691170568149893\n",
      "Num epoch: 44, Train_loss: 1.3764493414601102\n",
      "Num epoch: 45, Train_loss: 1.352446186557736\n",
      "Num epoch: 46, Train_loss: 1.3513351566799559\n",
      "Num epoch: 47, Train_loss: 1.3374853108055282\n",
      "Num epoch: 48, Train_loss: 1.338441840069386\n",
      "Num epoch: 49, Train_loss: 1.345490176364794\n",
      "Num epoch: 50, Train_loss: 1.3283985152555449\n",
      "Num epoch: 51, Train_loss: 1.310213746294558\n",
      "Num epoch: 52, Train_loss: 1.316549976681671\n",
      "Num epoch: 53, Train_loss: 1.3099206096256781\n",
      "Num epoch: 54, Train_loss: 1.3087367453430232\n",
      "Num epoch: 55, Train_loss: 1.2900324315255605\n",
      "Num epoch: 56, Train_loss: 1.2893810688197478\n",
      "Num epoch: 57, Train_loss: 1.2791007902029754\n",
      "Num epoch: 58, Train_loss: 1.2687450970873173\n",
      "Num epoch: 59, Train_loss: 1.293187466567848\n",
      "Num epoch: 60, Train_loss: 1.275584383065348\n",
      "Num epoch: 61, Train_loss: 1.2783731101585254\n",
      "Num epoch: 62, Train_loss: 1.2721330788297058\n",
      "Num epoch: 63, Train_loss: 1.2424395958241408\n",
      "Num epoch: 64, Train_loss: 1.260616169531325\n",
      "Num epoch: 65, Train_loss: 1.2331475624020152\n",
      "Num epoch: 66, Train_loss: 1.237395927422365\n",
      "Num epoch: 67, Train_loss: 1.2325967193457008\n",
      "Num epoch: 68, Train_loss: 1.2082432679991482\n",
      "Num epoch: 69, Train_loss: 1.2124217575524114\n",
      "Num epoch: 70, Train_loss: 1.1912482893695566\n",
      "Num epoch: 71, Train_loss: 1.2099253743786655\n",
      "Num epoch: 72, Train_loss: 1.2028441001648131\n",
      "Num epoch: 73, Train_loss: 1.2176142143471662\n",
      "Num epoch: 74, Train_loss: 1.2136009782021053\n",
      "Num epoch: 75, Train_loss: 1.21507006089384\n",
      "Num epoch: 76, Train_loss: 1.2047277321622738\n",
      "Num epoch: 77, Train_loss: 1.203837318820175\n",
      "Num epoch: 78, Train_loss: 1.2174316558870901\n",
      "Num epoch: 79, Train_loss: 1.2185381674595923\n",
      "Num epoch: 80, Train_loss: 1.2073902741927645\n",
      "Num epoch: 81, Train_loss: 1.1858628236894393\n",
      "Num epoch: 82, Train_loss: 1.1832354573422377\n",
      "Num epoch: 83, Train_loss: 1.1679200432479304\n",
      "Num epoch: 84, Train_loss: 1.187684930674309\n",
      "Num epoch: 85, Train_loss: 1.1703512421994917\n",
      "Num epoch: 86, Train_loss: 1.169492554889241\n",
      "Num epoch: 87, Train_loss: 1.1324742605759872\n",
      "Num epoch: 88, Train_loss: 1.149823905131621\n",
      "Num epoch: 89, Train_loss: 1.1490053162492264\n",
      "Num epoch: 90, Train_loss: 1.1499590918233245\n",
      "Num epoch: 91, Train_loss: 1.1537004101222748\n",
      "Num epoch: 92, Train_loss: 1.1599664685542643\n",
      "Num epoch: 93, Train_loss: 1.1945270382849957\n",
      "Num epoch: 94, Train_loss: 1.1471328820946844\n",
      "Num epoch: 95, Train_loss: 1.1637191642326505\n",
      "Num epoch: 96, Train_loss: 1.1695965441406628\n",
      "Num epoch: 97, Train_loss: 1.1742297108513202\n",
      "Num epoch: 98, Train_loss: 1.1559874769378764\n",
      "Num epoch: 99, Train_loss: 1.1522528371346816\n",
      "Num epoch: 100, Train_loss: 1.1703911745670652\n",
      "------------------------------\n",
      "Modelo 7 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.567106899921669\n",
      "Num epoch: 2, Train_loss: 1.5476032897631844\n",
      "Num epoch: 3, Train_loss: 1.5396768458566996\n",
      "Num epoch: 4, Train_loss: 1.5309194176084144\n",
      "Num epoch: 5, Train_loss: 1.5325896757456376\n",
      "Num epoch: 6, Train_loss: 1.5314710650241417\n",
      "Num epoch: 7, Train_loss: 1.5315093266724225\n",
      "Num epoch: 8, Train_loss: 1.5305210199597359\n",
      "Num epoch: 9, Train_loss: 1.5285642110330888\n",
      "Num epoch: 10, Train_loss: 1.5263069693701166\n",
      "Num epoch: 11, Train_loss: 1.5272318116941122\n",
      "Num epoch: 12, Train_loss: 1.5192715044710645\n",
      "Num epoch: 13, Train_loss: 1.5096256465174105\n",
      "Num epoch: 14, Train_loss: 1.5035080082649308\n",
      "Num epoch: 15, Train_loss: 1.497585229263884\n",
      "Num epoch: 16, Train_loss: 1.4943022639419674\n",
      "Num epoch: 17, Train_loss: 1.502121739560739\n",
      "Num epoch: 18, Train_loss: 1.4986684216797217\n",
      "Num epoch: 19, Train_loss: 1.4969309400205213\n",
      "Num epoch: 20, Train_loss: 1.4955906964647587\n",
      "Num epoch: 21, Train_loss: 1.5021773754023051\n",
      "Num epoch: 22, Train_loss: 1.4974470568619647\n",
      "Num epoch: 23, Train_loss: 1.506879714801255\n",
      "Num epoch: 24, Train_loss: 1.5111699916308783\n",
      "Num epoch: 25, Train_loss: 1.5042200021317669\n",
      "Num epoch: 26, Train_loss: 1.504256176882814\n",
      "------------------------------\n",
      "Modelo 8 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5686467682391143\n",
      "Num epoch: 2, Train_loss: 1.547439478393348\n",
      "Num epoch: 3, Train_loss: 1.5391634557682548\n",
      "Num epoch: 4, Train_loss: 1.5361247820839494\n",
      "Num epoch: 5, Train_loss: 1.5298341036596894\n",
      "Num epoch: 6, Train_loss: 1.5230931030478216\n",
      "Num epoch: 7, Train_loss: 1.5099827292249297\n",
      "Num epoch: 8, Train_loss: 1.5070046437587052\n",
      "Num epoch: 9, Train_loss: 1.4961819493302733\n",
      "Num epoch: 10, Train_loss: 1.4940635242169837\n",
      "Num epoch: 11, Train_loss: 1.5162734460645089\n",
      "Num epoch: 12, Train_loss: 1.507794014604456\n",
      "Num epoch: 13, Train_loss: 1.4952462864915852\n",
      "Num epoch: 14, Train_loss: 1.479833836287521\n",
      "Num epoch: 15, Train_loss: 1.464452728638779\n",
      "Num epoch: 16, Train_loss: 1.4487115990127637\n",
      "Num epoch: 17, Train_loss: 1.44834228537607\n",
      "Num epoch: 18, Train_loss: 1.4454676212378612\n",
      "Num epoch: 19, Train_loss: 1.4476509811698017\n",
      "Num epoch: 20, Train_loss: 1.4445887122869956\n",
      "Num epoch: 21, Train_loss: 1.4436516263318302\n",
      "Num epoch: 22, Train_loss: 1.4287176018908856\n",
      "Num epoch: 23, Train_loss: 1.4406577706810642\n",
      "Num epoch: 24, Train_loss: 1.420631912662888\n",
      "Num epoch: 25, Train_loss: 1.427483447157076\n",
      "Num epoch: 26, Train_loss: 1.4213048287782428\n",
      "Num epoch: 27, Train_loss: 1.4091505612954474\n",
      "Num epoch: 28, Train_loss: 1.3882835256567143\n",
      "Num epoch: 29, Train_loss: 1.3967543798817508\n",
      "Num epoch: 30, Train_loss: 1.3907678119458822\n",
      "Num epoch: 31, Train_loss: 1.3804430556116536\n",
      "Num epoch: 32, Train_loss: 1.3701041822614433\n",
      "Num epoch: 33, Train_loss: 1.3405168561112017\n",
      "Num epoch: 34, Train_loss: 1.3292420576941288\n",
      "Num epoch: 35, Train_loss: 1.3565287369125478\n",
      "Num epoch: 36, Train_loss: 1.328341172071022\n",
      "Num epoch: 37, Train_loss: 1.3033937125276753\n",
      "Num epoch: 38, Train_loss: 1.3089746728224092\n",
      "Num epoch: 39, Train_loss: 1.3055307215349727\n",
      "Num epoch: 40, Train_loss: 1.2624597744439239\n",
      "Num epoch: 41, Train_loss: 1.242603729657546\n",
      "Num epoch: 42, Train_loss: 1.2701104898253432\n",
      "Num epoch: 43, Train_loss: 1.2314025831079225\n",
      "Num epoch: 44, Train_loss: 1.2453247944036263\n",
      "Num epoch: 45, Train_loss: 1.2674733122041617\n",
      "Num epoch: 46, Train_loss: 1.23331471045323\n",
      "Num epoch: 47, Train_loss: 1.1926174833355614\n",
      "Num epoch: 48, Train_loss: 1.1810832634417776\n",
      "Num epoch: 49, Train_loss: 1.1723581235288656\n",
      "Num epoch: 50, Train_loss: 1.1691362496578424\n",
      "Num epoch: 51, Train_loss: 1.1684997889289994\n",
      "Num epoch: 52, Train_loss: 1.1604999278584738\n",
      "Num epoch: 53, Train_loss: 1.1351942142376212\n",
      "Num epoch: 54, Train_loss: 1.1178742353086961\n",
      "Num epoch: 55, Train_loss: 1.1560413090997301\n",
      "Num epoch: 56, Train_loss: 1.1062815700821698\n",
      "Num epoch: 57, Train_loss: 1.0638469972463758\n",
      "Num epoch: 58, Train_loss: 1.0915635708055513\n",
      "Num epoch: 59, Train_loss: 1.0993192577748183\n",
      "Num epoch: 60, Train_loss: 1.0677555260869867\n",
      "Num epoch: 61, Train_loss: 1.0215164939028598\n",
      "Num epoch: 62, Train_loss: 0.987507741596064\n",
      "Num epoch: 63, Train_loss: 1.0899174999825698\n",
      "Num epoch: 64, Train_loss: 1.0525521337035941\n",
      "Num epoch: 65, Train_loss: 1.0733712234063375\n",
      "Num epoch: 66, Train_loss: 1.095600190363399\n",
      "Num epoch: 67, Train_loss: 1.0828086498048142\n",
      "Num epoch: 68, Train_loss: 1.0544860878124944\n",
      "Num epoch: 69, Train_loss: 1.0712145898802894\n",
      "Num epoch: 70, Train_loss: 1.0596793195579541\n",
      "Num epoch: 71, Train_loss: 0.9996605878578033\n",
      "Num epoch: 72, Train_loss: 1.0013678770622905\n",
      "Num epoch: 73, Train_loss: 1.0075698626827916\n",
      "Num epoch: 74, Train_loss: 0.9899057732484032\n",
      "Num epoch: 75, Train_loss: 0.9958128028996639\n",
      "Num epoch: 76, Train_loss: 0.982814690143781\n",
      "Num epoch: 77, Train_loss: 0.944841052058216\n",
      "Num epoch: 78, Train_loss: 0.9685185912801989\n",
      "Num epoch: 79, Train_loss: 0.9803161861972646\n",
      "Num epoch: 80, Train_loss: 0.9850187481924674\n",
      "Num epoch: 81, Train_loss: 0.9557702060073839\n",
      "------------------------------\n",
      "Modelo 9 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5615210963018598\n",
      "Num epoch: 2, Train_loss: 1.5425064233441341\n",
      "Num epoch: 3, Train_loss: 1.5316400878660639\n",
      "Num epoch: 4, Train_loss: 1.52804433389836\n",
      "Num epoch: 5, Train_loss: 1.5251510054346729\n",
      "Num epoch: 6, Train_loss: 1.5210671066619188\n",
      "Num epoch: 7, Train_loss: 1.525626662766976\n",
      "Num epoch: 8, Train_loss: 1.517530375048204\n",
      "Num epoch: 9, Train_loss: 1.5141361389324877\n",
      "Num epoch: 10, Train_loss: 1.497826758729843\n",
      "Num epoch: 11, Train_loss: 1.487178155587472\n",
      "Num epoch: 12, Train_loss: 1.4785505139688528\n",
      "Num epoch: 13, Train_loss: 1.457351561629691\n",
      "Num epoch: 14, Train_loss: 1.4340148099416887\n",
      "Num epoch: 15, Train_loss: 1.413722071155031\n",
      "Num epoch: 16, Train_loss: 1.3953276246331785\n",
      "Num epoch: 17, Train_loss: 1.4098050719126456\n",
      "Num epoch: 18, Train_loss: 1.3895513171688365\n",
      "Num epoch: 19, Train_loss: 1.3720697263715487\n",
      "Num epoch: 20, Train_loss: 1.3762980315820992\n",
      "Num epoch: 21, Train_loss: 1.384542399186652\n",
      "Num epoch: 22, Train_loss: 1.3724049618017584\n",
      "Num epoch: 23, Train_loss: 1.3513196121842066\n",
      "Num epoch: 24, Train_loss: 1.3248339670187963\n",
      "Num epoch: 25, Train_loss: 1.341988256973877\n",
      "Num epoch: 26, Train_loss: 1.3406838165149648\n",
      "Num epoch: 27, Train_loss: 1.2967254082058162\n",
      "Num epoch: 28, Train_loss: 1.2892525533731982\n",
      "Num epoch: 29, Train_loss: 1.2930467286175271\n",
      "Num epoch: 30, Train_loss: 1.2944745790192804\n",
      "Num epoch: 31, Train_loss: 1.2523265166982689\n",
      "Num epoch: 32, Train_loss: 1.3191432832470487\n",
      "Num epoch: 33, Train_loss: 1.3138028105290107\n",
      "Num epoch: 34, Train_loss: 1.2805611705271562\n",
      "Num epoch: 35, Train_loss: 1.271536448853833\n",
      "Num epoch: 36, Train_loss: 1.2675453707439257\n",
      "Num epoch: 37, Train_loss: 1.255175463051121\n",
      "Num epoch: 38, Train_loss: 1.272196459702657\n",
      "Num epoch: 39, Train_loss: 1.2521374691803366\n",
      "Num epoch: 40, Train_loss: 1.2407193138127275\n",
      "Num epoch: 41, Train_loss: 1.2293647905963314\n",
      "Num epoch: 42, Train_loss: 1.2495788371651408\n",
      "Num epoch: 43, Train_loss: 1.2660439449615906\n",
      "Num epoch: 44, Train_loss: 1.2055155728163365\n",
      "Num epoch: 45, Train_loss: 1.205044648052712\n",
      "Num epoch: 46, Train_loss: 1.208771854553866\n",
      "Num epoch: 47, Train_loss: 1.1890953629039906\n",
      "Num epoch: 48, Train_loss: 1.1718267890873317\n",
      "Num epoch: 49, Train_loss: 1.1550734020827595\n",
      "Num epoch: 50, Train_loss: 1.1245428506935036\n",
      "Num epoch: 51, Train_loss: 1.1066438796837275\n",
      "Num epoch: 52, Train_loss: 1.1584442851365946\n",
      "Num epoch: 53, Train_loss: 1.151057503523291\n",
      "Num epoch: 54, Train_loss: 1.131362246476027\n",
      "Num epoch: 55, Train_loss: 1.1153781348788048\n",
      "Num epoch: 56, Train_loss: 1.1225912158078104\n",
      "Num epoch: 57, Train_loss: 1.1331827150959253\n",
      "Num epoch: 58, Train_loss: 1.1515466348919685\n",
      "Num epoch: 59, Train_loss: 1.1281351671507929\n",
      "Num epoch: 60, Train_loss: 1.1438861499509394\n",
      "Num epoch: 61, Train_loss: 1.103029458180911\n",
      "Num epoch: 62, Train_loss: 1.1017021057750285\n",
      "Num epoch: 63, Train_loss: 1.0546520830180703\n",
      "Num epoch: 64, Train_loss: 1.0675963417835657\n",
      "Num epoch: 65, Train_loss: 1.0929823484407097\n",
      "Num epoch: 66, Train_loss: 1.0525091500041153\n",
      "Num epoch: 67, Train_loss: 1.1134594863953091\n",
      "Num epoch: 68, Train_loss: 1.0562945931495575\n",
      "Num epoch: 69, Train_loss: 1.0694706899957067\n",
      "Num epoch: 70, Train_loss: 1.0872150863684833\n",
      "Num epoch: 71, Train_loss: 1.0333401477304198\n",
      "Num epoch: 72, Train_loss: 1.006130080087853\n",
      "Num epoch: 73, Train_loss: 1.005219992147458\n",
      "Num epoch: 74, Train_loss: 1.042644751184184\n",
      "Num epoch: 75, Train_loss: 1.0223543613882051\n",
      "Num epoch: 76, Train_loss: 0.9930729917727488\n",
      "Num epoch: 77, Train_loss: 1.0359199514547306\n",
      "Num epoch: 78, Train_loss: 1.0405144721917274\n",
      "Num epoch: 79, Train_loss: 1.0011687317435356\n",
      "Num epoch: 80, Train_loss: 0.9817606645376167\n",
      "Num epoch: 81, Train_loss: 0.998283206529467\n",
      "Num epoch: 82, Train_loss: 0.9949400800821304\n",
      "------------------------------\n",
      "Modelo 10 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.5631958798680876\n",
      "Num epoch: 2, Train_loss: 1.5427636697023208\n",
      "Num epoch: 3, Train_loss: 1.5299079447182362\n",
      "Num epoch: 4, Train_loss: 1.5275557927214554\n",
      "Num epoch: 5, Train_loss: 1.5234015308685438\n",
      "Num epoch: 6, Train_loss: 1.5067899957458732\n",
      "Num epoch: 7, Train_loss: 1.5080284044180372\n",
      "Num epoch: 8, Train_loss: 1.5065722981811613\n",
      "Num epoch: 9, Train_loss: 1.5006973154642271\n",
      "Num epoch: 10, Train_loss: 1.5028800981465462\n",
      "Num epoch: 11, Train_loss: 1.495872294771335\n",
      "Num epoch: 12, Train_loss: 1.4926221002010491\n",
      "Num epoch: 13, Train_loss: 1.4695024353379829\n",
      "Num epoch: 14, Train_loss: 1.4624569752736019\n",
      "Num epoch: 15, Train_loss: 1.4602152439455998\n",
      "Num epoch: 16, Train_loss: 1.4361041216611707\n",
      "Num epoch: 17, Train_loss: 1.4358095851785133\n",
      "Num epoch: 18, Train_loss: 1.4345208412973363\n",
      "Num epoch: 19, Train_loss: 1.4426232887636792\n",
      "Num epoch: 20, Train_loss: 1.4245697006104714\n",
      "Num epoch: 21, Train_loss: 1.4214216248965132\n",
      "Num epoch: 22, Train_loss: 1.4188811745768897\n",
      "Num epoch: 23, Train_loss: 1.4221265655551063\n",
      "Num epoch: 24, Train_loss: 1.396883801750457\n",
      "Num epoch: 25, Train_loss: 1.3848688808540768\n",
      "Num epoch: 26, Train_loss: 1.3794487065216028\n",
      "Num epoch: 27, Train_loss: 1.3777600315158103\n",
      "Num epoch: 28, Train_loss: 1.369928948298469\n",
      "Num epoch: 29, Train_loss: 1.352519278144183\n",
      "Num epoch: 30, Train_loss: 1.337499445190087\n",
      "Num epoch: 31, Train_loss: 1.328688260570212\n",
      "Num epoch: 32, Train_loss: 1.3439959592481654\n",
      "Num epoch: 33, Train_loss: 1.3312104507323754\n",
      "Num epoch: 34, Train_loss: 1.3333360649164412\n",
      "Num epoch: 35, Train_loss: 1.3500668769602628\n",
      "Num epoch: 36, Train_loss: 1.3354406515078965\n",
      "Num epoch: 37, Train_loss: 1.3253734953631413\n",
      "Num epoch: 38, Train_loss: 1.3081400180951441\n",
      "Num epoch: 39, Train_loss: 1.3197135906587976\n",
      "Num epoch: 40, Train_loss: 1.3056315415409858\n",
      "Num epoch: 41, Train_loss: 1.3280284888534877\n",
      "Num epoch: 42, Train_loss: 1.3082235524218808\n",
      "Num epoch: 43, Train_loss: 1.3171001916441327\n",
      "Num epoch: 44, Train_loss: 1.3450362857744131\n",
      "Num epoch: 45, Train_loss: 1.326701712744154\n",
      "Num epoch: 46, Train_loss: 1.2964828431055293\n",
      "Num epoch: 47, Train_loss: 1.2455582796767273\n",
      "Num epoch: 48, Train_loss: 1.2201717253615811\n",
      "Num epoch: 49, Train_loss: 1.1911244015709324\n",
      "Num epoch: 50, Train_loss: 1.2046016266474642\n",
      "Num epoch: 51, Train_loss: 1.194370938086791\n",
      "Num epoch: 52, Train_loss: 1.1719003832041721\n",
      "Num epoch: 53, Train_loss: 1.187305485190503\n",
      "Num epoch: 54, Train_loss: 1.1642883594540268\n",
      "Num epoch: 55, Train_loss: 1.1316613680754066\n",
      "Num epoch: 56, Train_loss: 1.1443984369045601\n",
      "Num epoch: 57, Train_loss: 1.1463461451478179\n",
      "Num epoch: 58, Train_loss: 1.129713131091278\n",
      "Num epoch: 59, Train_loss: 1.1225108329869475\n",
      "Num epoch: 60, Train_loss: 1.1169903226329079\n",
      "Num epoch: 61, Train_loss: 1.1240049171510105\n",
      "Num epoch: 62, Train_loss: 1.0863239500587647\n",
      "Num epoch: 63, Train_loss: 1.1811783629385897\n",
      "Num epoch: 64, Train_loss: 1.1442615816516466\n",
      "Num epoch: 65, Train_loss: 1.1475671255469269\n",
      "Num epoch: 66, Train_loss: 1.1154806481674615\n",
      "Num epoch: 67, Train_loss: 1.0975421687966909\n",
      "Num epoch: 68, Train_loss: 1.1406666014480646\n",
      "Num epoch: 69, Train_loss: 1.1655103226799182\n",
      "Num epoch: 70, Train_loss: 1.1581853866734837\n",
      "Num epoch: 71, Train_loss: 1.1298143555465487\n",
      "Num epoch: 72, Train_loss: 1.1212822167734346\n",
      "------------------------------\n",
      "Modelo 11 done\n",
      "------------------------------\n",
      "Num epoch: 1, Train_loss: 1.558960279299383\n",
      "Num epoch: 2, Train_loss: 1.5338059377639355\n",
      "Num epoch: 3, Train_loss: 1.5285585650638291\n",
      "Num epoch: 4, Train_loss: 1.5182703798017523\n",
      "Num epoch: 5, Train_loss: 1.5123501064384521\n",
      "Num epoch: 6, Train_loss: 1.5020309851670706\n",
      "Num epoch: 7, Train_loss: 1.5039863679515748\n",
      "Num epoch: 8, Train_loss: 1.5034256128958592\n",
      "Num epoch: 9, Train_loss: 1.488899071824423\n",
      "Num epoch: 10, Train_loss: 1.4583440845686624\n",
      "Num epoch: 11, Train_loss: 1.448918095616756\n",
      "Num epoch: 12, Train_loss: 1.4407570298581087\n",
      "Num epoch: 13, Train_loss: 1.4121454134510973\n",
      "Num epoch: 14, Train_loss: 1.4143807242491242\n",
      "Num epoch: 15, Train_loss: 1.4200720562878422\n",
      "Num epoch: 16, Train_loss: 1.4205774704705498\n",
      "Num epoch: 17, Train_loss: 1.408270352695559\n",
      "Num epoch: 18, Train_loss: 1.4083172902779033\n",
      "Num epoch: 19, Train_loss: 1.3758860541494915\n",
      "Num epoch: 20, Train_loss: 1.3699088707920304\n",
      "Num epoch: 21, Train_loss: 1.3601481956376915\n",
      "Num epoch: 22, Train_loss: 1.3496577363199047\n",
      "Num epoch: 23, Train_loss: 1.3198934915960996\n",
      "Num epoch: 24, Train_loss: 1.3106812743728051\n",
      "Num epoch: 25, Train_loss: 1.2948296164977176\n",
      "Num epoch: 26, Train_loss: 1.2891475998911617\n",
      "Num epoch: 27, Train_loss: 1.2785377464717254\n",
      "Num epoch: 28, Train_loss: 1.2719746592393895\n",
      "Num epoch: 29, Train_loss: 1.2951973546676983\n",
      "Num epoch: 30, Train_loss: 1.2633496755936482\n",
      "Num epoch: 31, Train_loss: 1.2554074059827467\n",
      "Num epoch: 32, Train_loss: 1.2801505343464725\n",
      "Num epoch: 33, Train_loss: 1.3132944475234696\n",
      "Num epoch: 34, Train_loss: 1.29629616882559\n",
      "Num epoch: 35, Train_loss: 1.2674048711780483\n",
      "Num epoch: 36, Train_loss: 1.2611492156464048\n",
      "Num epoch: 37, Train_loss: 1.24172314966144\n",
      "Num epoch: 38, Train_loss: 1.2462388239835291\n",
      "Num epoch: 39, Train_loss: 1.251068102377713\n",
      "Num epoch: 40, Train_loss: 1.2398943031279832\n",
      "Num epoch: 41, Train_loss: 1.2477825664728759\n",
      "Num epoch: 42, Train_loss: 1.2349053711491145\n",
      "Num epoch: 43, Train_loss: 1.2299326681894465\n",
      "Num epoch: 44, Train_loss: 1.2285718462465285\n",
      "Num epoch: 45, Train_loss: 1.2452155718254545\n",
      "Num epoch: 46, Train_loss: 1.231465824988758\n",
      "Num epoch: 47, Train_loss: 1.2187740989203106\n",
      "Num epoch: 48, Train_loss: 1.2378428342481114\n",
      "Num epoch: 49, Train_loss: 1.2350549821744448\n",
      "Num epoch: 50, Train_loss: 1.1981321030525667\n",
      "Num epoch: 51, Train_loss: 1.1425228298349022\n",
      "Num epoch: 52, Train_loss: 1.1686870823404352\n",
      "Num epoch: 53, Train_loss: 1.1311815458017893\n",
      "Num epoch: 54, Train_loss: 1.0904007139497902\n",
      "Num epoch: 55, Train_loss: 1.049213798859474\n",
      "Num epoch: 56, Train_loss: 1.0418647265048444\n",
      "Num epoch: 57, Train_loss: 1.0387517204869174\n",
      "Num epoch: 58, Train_loss: 1.0188500136547842\n",
      "Num epoch: 59, Train_loss: 1.036890832213984\n",
      "Num epoch: 60, Train_loss: 1.0645312674112795\n",
      "Num epoch: 61, Train_loss: 1.053058702783048\n",
      "Num epoch: 62, Train_loss: 1.0657333590968885\n",
      "Num epoch: 63, Train_loss: 1.1106183081298904\n",
      "Num epoch: 64, Train_loss: 1.096305107522825\n",
      "Num epoch: 65, Train_loss: 1.10656977574647\n",
      "Num epoch: 66, Train_loss: 1.0868872530699512\n",
      "Num epoch: 67, Train_loss: 1.0736437745671297\n",
      "Num epoch: 68, Train_loss: 1.071987524508561\n",
      "Num epoch: 69, Train_loss: 1.0573152114072564\n",
      "------------------------------\n",
      "Modelo 12 done\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "DROPOUT = 0.1\n",
    "#He definido un directamente el embed_im, att_h, d_ff porque solo busco cual es el mejor tamaño posible.\n",
    "#En otro apartado determinaré cuales son los mejores hiperparametros\n",
    "POSSIBLE_VALUES = [(512,8,2048), (768,12,3072)]\n",
    "EMBED_DIM, ATT_HEADS, D_FF = POSSIBLE_VALUES[0]\n",
    "\n",
    "\n",
    "print(\"VOCAB SIZE\")\n",
    "result_vocab = pd.read_excel(SAVE_PATH_R)\n",
    "#vocab_ize = 118 es el minimo vocabulario\n",
    "VOCAB_SIZE = [118, 1000, 2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000, 25000, 30000, 35000] \n",
    "for vocab_size in VOCAB_SIZE:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if vocab_size in result_vocab['Vocab_size'].values:\n",
    "        continue\n",
    "    \n",
    "    vocabM = MyVocabModel(df_rev['Review'])\n",
    "    vocab = vocabM.create_vocab(vocab_size)\n",
    "    with open(NORMAL_PATH, \"w\") as archivo:\n",
    "        for word in vocab:\n",
    "            archivo.write(word + \"\\n\")\n",
    "\n",
    "    tokenizer = MyTokenizer(NORMAL_PATH)\n",
    "\n",
    "    dataset_train_torch = DataLoaderBert(df_train['Review'].to_list(), df_train['Score_G'].to_list(),tokenizer, MAX_LEN)\n",
    "    train_dataloader = DataLoader(dataset_train_torch, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    dataset_val_torch = DataLoaderBert(df_val['Review'].to_list(), df_val['Score_G'].to_list(),tokenizer, MAX_LEN)\n",
    "    val_dataloader = DataLoader(dataset_val_torch, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    model = MyBert(vocab_size, MAX_LEN, EMBED_DIM, ATT_HEADS, D_FF, DROPOUT, NUM_LABELS, device, N=1).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "    best_acc = 0\n",
    "    early_stopping = EarlyStopping()\n",
    "\n",
    "    num_epochs = 1000\n",
    "    inicio = time.perf_counter() \n",
    "    for epoch in range(num_epochs): \n",
    "       train_loss_epoch, train_acc_epoch = model_train(model, train_dataloader, loss_fn,optimizer, len(df_train), device)\n",
    "       train_loss.append(train_loss_epoch)\n",
    "       train_acc.append(train_acc_epoch)\n",
    "\n",
    "       val_loss_epoch, val_acc_epoch, _ = model_eval(model, val_dataloader, loss_fn, len(df_val), device)\n",
    "       val_loss.append(val_loss_epoch)\n",
    "       val_acc.append(val_acc_epoch)\n",
    "\n",
    "       print(f\"Num epoch: {epoch+1}, Train_loss: {train_loss_epoch}\")\n",
    "\n",
    "       if early_stopping(val_loss_epoch, model):\n",
    "          break\n",
    "    \n",
    "    final = time.perf_counter()\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Modelo {len(result_vocab)} done\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    res = {\n",
    "        'Vocab_size': vocab_size,\n",
    "        'Time': int(round(final - inicio,2)/60),\n",
    "        'Train_loss': sorted(train_loss)[0],\n",
    "        'Train_acc': sorted(train_acc, reverse=True)[0].item(),\n",
    "        'Val_loss': sorted(val_loss)[0],\n",
    "        'Val_acc': sorted(val_acc, reverse=True)[0].item()\n",
    "    }\n",
    "\n",
    "    result_vocab.loc[len(result_vocab)] = res\n",
    "    result_vocab.to_excel(SAVE_PATH_R, index=False)\n",
    "    \n",
    "\n",
    "len(result_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SAVE_PATH_V \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpardir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m BEST_VOCAB_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[1;32m      4\u001b[0m vocabM \u001b[38;5;241m=\u001b[39m MyVocabModel(df_revisado[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "SAVE_PATH_V = os.path.join(os.pardir, \"vocab_file.txt\")\n",
    "BEST_VOCAB_SIZE = 20000\n",
    "\n",
    "vocabM = MyVocabModel(df_revisado['Review'])\n",
    "vocab = vocabM.create_vocab(BEST_VOCAB_SIZE)\n",
    "\"\"\"\"\"\" \n",
    "with open(SAVE_PATH_V, \"w\") as archivo:\n",
    "    for word in vocab:\n",
    "        archivo.write(word + \"\\n\")\n",
    "       \n",
    "print(\"Vocab done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
